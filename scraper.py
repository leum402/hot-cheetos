# -*- coding: utf-8 -*-
"""
ì‹¤ì‹œê°„ ê¸‰ë“±ì£¼ + ì˜¤ëŠ˜ì ë‰´ìŠ¤ 2ì¤„ ìš”ì•½(ğŸŸ¢/ğŸ”´) - ìºì‹± & ë§í¬ ì œê³µ ë²„ì „
- Google News RSS(24h) í—¤ë“œë¼ì¸ â†’ GPTê°€ í˜¸ì¬/ì•…ì¬ì™€ ê·¼ê±° ì¸ë±ìŠ¤ ë°˜í™˜ â†’ ë§í¬ ë§¤í•‘
- ë™ì¼ ì¢…ëª©: 1ì‹œê°„ ìºì‹œ (news_cache.json)
- ìºì‹œì— ê³¼ê±° ë¬¸ìì—´ í¬ë§· ìˆìœ¼ë©´ MISS ì²˜ë¦¬í•˜ì—¬ ë§í¬ ë¶™ê²Œ ì¬ìƒì„±
- APIë¡œ bullish_url/bearish_url/sources í¬í•¨
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

import os
import re
import time
import json
import random
import xml.etree.ElementTree as ET
import requests

from urllib.parse import quote_plus
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple, List
from dotenv import load_dotenv

# =========================
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# =========================
load_dotenv()

API_URL = os.getenv("API_URL", "http://127.0.0.1:5001/api/update")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")  # .envì—ì„œ ì¡°ì ˆ ê°€ëŠ¥
OPENAI_TIMEOUT = float(os.getenv("OPENAI_TIMEOUT", "12"))
OPENAI_RETRIES = int(os.getenv("OPENAI_RETRIES", "2"))
MAX_LINE_LEN = int(os.getenv("NEWS_MAX_LINE_LEN", "60"))
CACHE_DURATION_MINUTES = int(os.getenv("NEWS_CACHE_MINUTES", "60"))

# =========================
# ìºì‹œ
# =========================
class NewsCache:
    """
    value(dict) ìŠ¤í‚¤ë§ˆ:
    {
      "summary": "ğŸŸ¢ ...\nğŸ”´ ...",
      "bullish_url": str,
      "bearish_url": str,
      "sources": [{"title":..., "link":..., "published":...}, ...]
    }
    """
    def __init__(self, cache_duration_minutes: int = 60):
        self.cache: Dict[str, Tuple[dict|str, datetime]] = {}
        self.cache_duration = timedelta(minutes=cache_duration_minutes)
        self.hit_count = 0
        self.miss_count = 0
        self.cache_file = "news_cache.json"
        self.load_cache()

    def load_cache(self):
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                now = datetime.now()
                for stock, (value, ts_str) in data.items():
                    ts = datetime.fromisoformat(ts_str)
                    if now - ts < self.cache_duration:
                        self.cache[stock] = (value, ts)
                print(f"ğŸ“¦ ìºì‹œ ë¡œë“œ: {len(self.cache)}ê°œ ì¢…ëª©")
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def save_cache(self):
        try:
            data = {stock: (value, ts.isoformat()) for stock, (value, ts) in self.cache.items()}
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get(self, stock_name: str) -> Optional[dict]:
        """ë¬¸ìì—´ í¬ë§·(ì˜› ìºì‹œ)ì€ MISSë¡œ ì²˜ë¦¬ â†’ ë§í¬ ë¶™ë„ë¡ ì¬ìš”ì•½ ìœ ë„"""
        if stock_name in self.cache:
            value, cached_time = self.cache[stock_name]
            if datetime.now() - cached_time < self.cache_duration:
                if isinstance(value, str):
                    self.miss_count += 1
                    return None
                self.hit_count += 1
                remaining = self.cache_duration - (datetime.now() - cached_time)
                print(f"  ğŸ’¾ ìºì‹œ ì‚¬ìš©: {stock_name} (ë‚¨ì€ {remaining.seconds//60}ë¶„)")
                return value
            else:
                del self.cache[stock_name]
        self.miss_count += 1
        return None

    def set(self, stock_name: str, summary_or_obj):
        value = {"summary": summary_or_obj, "bullish_url": "", "bearish_url": "", "sources": []} \
                if isinstance(summary_or_obj, str) else summary_or_obj
        self.cache[stock_name] = (value, datetime.now())
        self.save_cache()
        print(f"  ğŸ’¾ ìºì‹œ ì €ì¥: {stock_name}")

    def get_stats(self) -> str:
        total = self.hit_count + self.miss_count
        hit_rate = (self.hit_count / total * 100) if total else 0
        return f"ìºì‹œ ì ì¤‘ë¥ : {hit_rate:.1f}% (ì ì¤‘:{self.hit_count}, ë¯¸ìŠ¤:{self.miss_count})"

    def cleanup(self):
        now = datetime.now()
        expired = [stock for stock, (_, ts) in self.cache.items() if now - ts >= self.cache_duration]
        for stock in expired:
            del self.cache[stock]
        if expired:
            print(f"ğŸ—‘ï¸ ë§Œë£Œ ìºì‹œ ì •ë¦¬: {len(expired)}ê°œ")
            self.save_cache()

news_cache = NewsCache(CACHE_DURATION_MINUTES)

# =========================
# ìœ í‹¸/ì‹œì¥ì‹œê°„
# =========================
def is_market_hours() -> bool:
    """í‰ì¼ 09:00~15:30 KST"""
    now = datetime.now()
    if now.weekday() >= 5:
        return False
    t = now.time()
    o = datetime.strptime("09:00", "%H:%M").time()
    c = datetime.strptime("15:30", "%H:%M").time()
    return o <= t <= c

def get_update_interval() -> int:
    """ì¥ì¤‘ 10ì´ˆ, ì¥ì™¸ 60ì´ˆ"""
    return 10 if is_market_hours() else 60

def _trim_line(s: str, max_len: int = MAX_LINE_LEN) -> str:
    s = re.sub(r"\s+", " ", s or "").strip()
    return (s[:max_len] + "â€¦") if len(s) > max_len else s

def _force_two_lines(text: str) -> str:
    lines = [ln.strip() for ln in (text or "").splitlines() if ln.strip()]
    green = next((l for l in lines if l.startswith("ğŸŸ¢")), None)
    red   = next((l for l in lines if l.startswith("ğŸ”´")), None)
    if not green:
        first = lines[0] if lines else "íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ"
        green = f"ğŸŸ¢ í˜¸ì¬: {_trim_line(first)}"
    if not red:
        second = lines[1] if len(lines) > 1 else "íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ"
        red = f"ğŸ”´ ì•…ì¬: {_trim_line(second)}"
    return f"{_trim_line(green)}\n{_trim_line(red)}"

# =========================
# í† ìŠ¤ í˜ì´ì§€ ìƒíƒœ ì²´í¬
# =========================
def check_page_health(driver) -> bool:
    try:
        result = driver.execute_script("""
            return {
                ready: document.readyState === 'complete',
                hasData: document.querySelectorAll('tr[data-tossinvest-log="RankingListRow"]').length > 0
            }
        """)
        return bool(result.get('ready')) and bool(result.get('hasData'))
    except Exception:
        return False

# =========================
# ë‰´ìŠ¤ ìˆ˜ì§‘ (Google News RSS)
# =========================
def fetch_google_news_today(stock_name: str, max_items: int = 6) -> List[dict]:
    q = quote_plus(f"{stock_name} when:1d")
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        resp = requests.get(url, headers=headers, timeout=4)
        resp.raise_for_status()
    except Exception:
        return []
    items: List[dict] = []
    try:
        root = ET.fromstring(resp.content)
        for it in root.findall(".//item"):
            title_el = it.find("title")
            link_el  = it.find("link")
            pub_el   = it.find("{http://purl.org/dc/elements/1.1/}date") or it.find("pubDate")
            title = (title_el.text if title_el is not None else "") or ""
            link  = (link_el.text  if link_el  is not None else "") or ""
            pub   = (pub_el.text   if pub_el   is not None else "") or ""
            title, link, pub = title.strip(), link.strip(), pub.strip()
            if any(k in title for k in ["ë£¨ë¨¸", "ì¶”ì •", "ì†Œë¬¸", "ì „ë§ë§Œ", "ì˜ˆìƒë§Œ"]):
                continue
            if title:
                items.append({"title": title, "link": link, "published": pub})
            if len(items) >= max_items:
                break
    except Exception:
        return []
    return items

def format_headlines_for_prompt(headlines: List[dict]) -> str:
    if not headlines:
        return "í—¤ë“œë¼ì¸ ì—†ìŒ"
    return "\n".join(f"{i}. {h.get('title','')} ({h.get('link','')})" for i, h in enumerate(headlines, 1))

# =========================
# ìš”ì•½(GPT or ê·œì¹™) â†’ ë§í¬ ë§¤í•‘
# =========================
def summarize_with_gpt_from_headlines(stock_name: str, rate_text: str, headlines: List[dict]) -> dict:
    """
    ë°˜í™˜:
    {
      "summary": "ğŸŸ¢ ...\nğŸ”´ ...",
      "bullish_url": str,
      "bearish_url": str,
      "sources": [ {title,link,published}, ... ]
    }
    """
    def _wrap(summary_text: str, bull_idx: int|None = None, bear_idx: int|None = None) -> dict:
        bull_url = headlines[bull_idx-1]["link"] if headlines and bull_idx and 1 <= bull_idx <= len(headlines) else ""
        bear_url = headlines[bear_idx-1]["link"] if headlines and bear_idx and 1 <= bear_idx <= len(headlines) else ""
        return {
            "summary": _force_two_lines(summary_text),
            "bullish_url": bull_url,
            "bearish_url": bear_url,
            "sources": headlines[:]
        }

    # ê·œì¹™ê¸°ë°˜(í‚¤ ì—†ìœ¼ë©´)
    if not OPENAI_API_KEY:
        base = rule_based_summary(stock_name, rate_text, headlines)
        # ê¸°ë³¸ê°’: í˜¸ì¬=1ë²ˆ, ì•…ì¬=2ë²ˆ(ì—†ìœ¼ë©´ 1ë²ˆ)
        return _wrap(base, bull_idx=1 if len(headlines)>=1 else None,
                          bear_idx=2 if len(headlines)>=2 else (1 if len(headlines)>=1 else None))

    # OpenAI SDK
    try:
        from openai import OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception:
        base = rule_based_summary(stock_name, rate_text, headlines)
        return _wrap(base, bull_idx=1 if len(headlines)>=1 else None,
                          bear_idx=2 if len(headlines)>=2 else (1 if len(headlines)>=1 else None))

    system = (
        "ë„ˆëŠ” í•œêµ­ ì£¼ì‹ ë‰´ìŠ¤ ìš”ì•½ ë„ìš°ë¯¸ë‹¤. ë°˜ë“œì‹œ 'ì˜¤ëŠ˜/ìµœê·¼ 24ì‹œê°„' ì‹¤ì œ ê¸°ì‚¬ ì œëª©ë§Œ ê·¼ê±°ë¡œ í•œë‹¤. "
        "ì¶œë ¥ì€ ì •í™•íˆ ë‘ ì¤„(ğŸŸ¢/ğŸ”´), ê° í•œ ë¬¸ì¥, 60ì ì´ë‚´."
    )
    user = (
        f"ì¢…ëª©: {stock_name}\në“±ë½ë¥ : {rate_text}\n"
        f"ì˜¤ëŠ˜ì í—¤ë“œë¼ì¸(ë²ˆí˜¸ ë¶€ì—¬ë¨):\n{format_headlines_for_prompt(headlines)}\n\n"
        "ì•„ë˜ JSONë§Œ ì¶œë ¥í•˜ë¼(í…ìŠ¤íŠ¸ ì„¤ëª… ê¸ˆì§€):\n"
        "{\n"
        '  "bullish": "í˜¸ì¬ í•œ ë¬¸ì¥",\n'
        '  "bearish": "ì•…ì¬ í•œ ë¬¸ì¥",\n'
        '  "bullish_idx": 1,\n'
        '  "bearish_idx": 2\n'
        "}\n"
        "ì¸ë±ìŠ¤ëŠ” ìœ„ í—¤ë“œë¼ì¸ ë²ˆí˜¸(1ë¶€í„°). ê·¼ê±°ê°€ ì—†ìœ¼ë©´ í•´ë‹¹ *_idxëŠ” ìƒëµ."
    )

    last_err = None
    for attempt in range(OPENAI_RETRIES + 1):
        try:
            resp = client.chat.completions.create(
                model=OPENAI_MODEL,
                temperature=0.2,
                max_tokens=160,
                timeout=OPENAI_TIMEOUT,
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": user},
                ],
            )
            out = (resp.choices[0].message.content or "").strip()
            # ì½”ë“œë¸”ë¡ ì œê±°
            if out.startswith("```"):
                out = out.strip("`").strip()
                if out.startswith("json"):
                    out = out[4:].strip()
            try:
                data = json.loads(out)
            except Exception:
                # JSON ì‹¤íŒ¨ â†’ í…ìŠ¤íŠ¸ 2ì¤„ ê°•ì œ + ê¸°ë³¸ ì¸ë±ìŠ¤
                return _wrap(out,
                             bull_idx=1 if len(headlines)>=1 else None,
                             bear_idx=2 if len(headlines)>=2 else (1 if len(headlines)>=1 else None))

            green = _trim_line(f"ğŸŸ¢ í˜¸ì¬: {data.get('bullish','íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ')}")
            red   = _trim_line(f"ğŸ”´ ì•…ì¬: {data.get('bearish','íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ')}")
            bull_idx = data.get("bullish_idx") or (1 if len(headlines)>=1 else None)
            bear_idx = data.get("bearish_idx") or (2 if len(headlines)>=2 else (bull_idx if bull_idx else None))
            return _wrap(f"{green}\n{red}", bull_idx=bull_idx, bear_idx=bear_idx)

        except Exception as e:
            last_err = e
            time.sleep(0.6 * (attempt + 1))

    base = rule_based_summary(stock_name, rate_text, headlines)
    return _wrap(base, bull_idx=1 if len(headlines)>=1 else None,
                      bear_idx=2 if len(headlines)>=2 else (1 if len(headlines)>=1 else None))

def rule_based_summary(stock_name: str, rate_text: str, headlines: List[dict]) -> str:
    if headlines:
        top = _trim_line(headlines[0]["title"])
        green = f"ğŸŸ¢ í˜¸ì¬: {top}"
        red = "ğŸ”´ ì•…ì¬: ë‹¨ê¸° ë³€ë™ì„±/ì°¨ìµì‹¤í˜„ ì£¼ì˜"
    else:
        green = "ğŸŸ¢ í˜¸ì¬: íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ"
        red = "ğŸ”´ ì•…ì¬: íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ"
    return f"{_trim_line(green)}\n{_trim_line(red)}"

# =========================
# GPT í˜¸ì¶œ(ìºì‹œ ì‚¬ìš©)
# =========================
def get_gpt_news_with_context_cached(stock_name: str, current_rate: str) -> dict:
    cached = news_cache.get(stock_name)
    if cached:
        return cached
    print(f"  ğŸ”„ ìƒˆë¡œìš´ ë‰´ìŠ¤ ìš”ì²­: {stock_name}")
    headlines = fetch_google_news_today(stock_name)
    result = summarize_with_gpt_from_headlines(stock_name, current_rate, headlines)
    news_cache.set(stock_name, result)
    return result

# =========================
# í¬ë¡¬ ë“œë¼ì´ë²„
# =========================
def setup_driver():
    options = Options()
    options.add_argument('--headless')  # â† ì£¼ì„ í•´ì œ í•„ìˆ˜!
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--window-size=1920,1080')
    
    # Docker í™˜ê²½ì„ ìœ„í•œ ì¶”ê°€ ì˜µì…˜
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    # ChromeDriver ê²½ë¡œ ìë™ ì„¤ì •
    try:
        service = Service(ChromeDriverManager().install())
    except:
        # Docker í™˜ê²½ì—ì„œ ì‹¤íŒ¨ ì‹œ ì§ì ‘ ê²½ë¡œ ì§€ì •
        service = Service('/usr/bin/chromedriver')
    
    driver = webdriver.Chrome(service=service, options=options)
    return driver

# =========================
# í† ìŠ¤ íŒŒì‹±
# =========================
def parse_toss_data_with_cache(soup, use_gpt=True):
    stocks = []
    rows = soup.select('tr[data-tossinvest-log="RankingListRow"]')
    if not rows:
        print("âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        rows = soup.select('tbody tr')[:10]

    print(f"\nğŸ“Š {len(rows)}ê°œ ì¢…ëª© ë°œê²¬.")
    print(f"ğŸ“ˆ {news_cache.get_stats()}\n")

    for i, row in enumerate(rows[:10], 1):
        try:
            # ì¢…ëª©ëª…
            name = None
            tag = row.select_one('span[class*="60z0ev1"]')
            if tag: name = tag.get_text(strip=True)
            if not name:
                for sp in row.select('span'):
                    tx = sp.get_text(strip=True)
                    if tx and '%' not in tx and 'ì›' not in tx and ',' not in tx and 2 <= len(tx) <= 20:
                        name = tx; break

            # ê°€ê²©/ë“±ë½ë¥ 
            price_spans = row.select('span._1p5yqoh0')
            if not name: name = f"ì¢…ëª©{i}"
            price = "ê°€ê²© í™•ì¸ì¤‘"; rate = "+0.00%"
            if len(price_spans) >= 2:
                price = price_spans[0].get_text(strip=True)
                rate  = price_spans[1].get_text(strip=True)
            elif len(price_spans) == 1:
                price = price_spans[0].get_text(strip=True)

            print(f"  {i}. {name} - {price} ({rate})")

            # ë‰´ìŠ¤(ìºì‹œ)
            if use_gpt:
                result = get_gpt_news_with_context_cached(name, rate)
            else:
                cached = news_cache.get(name)
                if cached: result = cached
                else:
                    result = {"summary": get_simple_summary(name), "bullish_url":"", "bearish_url":"", "sources":[]}
                    news_cache.set(name, result)

            stocks.append({
                "rank": i,
                "name": name,
                "price": price,
                "rate": rate,
                "summary": result["summary"],
                "bullish_url": result.get("bullish_url", ""),
                "bearish_url": result.get("bearish_url", ""),
                "sources": result.get("sources", [])
            })

            for ln in result["summary"].split('\n'):
                print(f"      {ln}")
            if result.get("bullish_url"): print(f"      â†— í˜¸ì¬ ë§í¬: {result['bullish_url']}")
            if result.get("bearish_url"): print(f"      â†— ì•…ì¬ ë§í¬: {result['bearish_url']}")
            print()

        except Exception as e:
            print(f"  âŒ {i}ë²ˆ ì¢…ëª© ì˜¤ë¥˜: {e}")
            stocks.append({
                "rank": i,
                "name": f"ì¢…ëª©{i}",
                "price": "í™•ì¸ì¤‘",
                "rate": "+0.00%",
                "summary": get_simple_summary(f"ì¢…ëª©{i}"),
                "bullish_url": "",
                "bearish_url": "",
                "sources": []
            })

    return stocks

# =========================
# í…ŒìŠ¤íŠ¸ ë°ì´í„°
# =========================
def generate_test_data_with_cache():
    test_stocks = [
        {"name": "ì‚¼ì„±ì „ì", "price": "87,500ì›", "rate": "+29.95%"},
        {"name": "SKí•˜ì´ë‹‰ìŠ¤", "price": "142,300ì›", "rate": "+25.32%"},
        {"name": "ì¹´ì¹´ì˜¤", "price": "58,900ì›", "rate": "+21.24%"},
        {"name": "ë„¤ì´ë²„", "price": "185,200ì›", "rate": "+18.56%"},
        {"name": "í˜„ëŒ€ì°¨", "price": "245,000ì›", "rate": "+15.87%"},
        {"name": "LGí™”í•™", "price": "485,000ì›", "rate": "+12.65%"},
        {"name": "í¬ìŠ¤ì½”í™€ë”©ìŠ¤", "price": "392,500ì›", "rate": "+10.93%"},
        {"name": "ì‚¼ì„±SDI", "price": "425,000ì›", "rate": "+9.54%"},
        {"name": "ì…€íŠ¸ë¦¬ì˜¨", "price": "178,900ì›", "rate": "+8.82%"},
        {"name": "ê¸°ì•„", "price": "115,200ì›", "rate": "+7.95%"}
    ]

    stocks = []
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {len(test_stocks)}ê°œ ì¢…ëª©")
    print(f"ğŸ“ˆ {news_cache.get_stats()}\n")

    for i, st in enumerate(test_stocks, 1):
        print(f"  {i}. {st['name']} - {st['price']} ({st['rate']})")
        result = get_gpt_news_with_context_cached(st["name"], st["rate"])
        stocks.append({
            "rank": i,
            "name": st["name"],
            "price": st["price"],
            "rate": st["rate"],
            "summary": result["summary"],
            "bullish_url": result.get("bullish_url", ""),
            "bearish_url": result.get("bearish_url", ""),
            "sources": result.get("sources", [])
        })
        for ln in result["summary"].split('\n'):
            print(f"      {ln}")
        if result.get("bullish_url"): print(f"      â†— í˜¸ì¬ ë§í¬: {result['bullish_url']}")
        if result.get("bearish_url"): print(f"      â†— ì•…ì¬ ë§í¬: {result['bearish_url']}")
        print()

    return stocks

# =========================
# ë‹¨ìˆœ ìš”ì•½(ë¹„ìƒ)
# =========================
def get_simple_summary(stock_name):
    samples = [
        f"ğŸŸ¢ í˜¸ì¬: {stock_name} ê±°ë˜ëŸ‰ ê¸‰ì¦\nğŸ”´ ì•…ì¬: ë‹¨ê¸° ê³¼ì—´ ì£¼ì˜",
        f"ğŸŸ¢ í˜¸ì¬: {stock_name} ê¸°ê´€ ìˆœë§¤ìˆ˜ ìœ ì…\nğŸ”´ ì•…ì¬: ì°¨ìµì‹¤í˜„ ë§¤ë¬¼ ê°€ëŠ¥",
        f"ğŸŸ¢ í˜¸ì¬: {stock_name} ì—…ì¢… ê°•ì„¸ ë™ì¡°\nğŸ”´ ì•…ì¬: ë³€ë™ì„± í™•ëŒ€ ì£¼ì˜"
    ]
    return random.choice(samples)

# =========================
# API ì „ì†¡
# =========================
def send_to_api(data):
    try:
        resp = requests.post(API_URL, json=data, timeout=5)
        if resp.status_code == 200:
            print("âœ… API ì „ì†¡ ì„±ê³µ")
            return True
        else:
            print(f"âŒ API ì‘ë‹µ ì½”ë“œ: {resp.status_code}")
    except Exception as e:
        print(f"âŒ API ì „ì†¡ ì‹¤íŒ¨: {e}")
    return False

# =========================
# ë©”ì¸
# =========================
def main():
    print("=" * 50)
    print("ğŸš€ ì‹¤ì‹œê°„ ê¸‰ë“±ì£¼ + 2ì¤„ ë‰´ìŠ¤ & ë§í¬ (ìºì‹±)")
    print(f"â° ìºì‹œ ìœ ì§€: {CACHE_DURATION_MINUTES}ë¶„")
    print("=" * 50)

    if OPENAI_API_KEY:
        print(f"âœ… OpenAI API í‚¤ í™•ì¸ë¨: {OPENAI_API_KEY[:10]}â€¦  (ëª¨ë¸: {OPENAI_MODEL})")
    else:
        print("âš ï¸ OpenAI API í‚¤ ì—†ìŒ â†’ ê·œì¹™ê¸°ë°˜ ìš”ì•½ ì‚¬ìš©")

    print("\nëª¨ë“œ ì„ íƒ:")
    print("1. í† ìŠ¤ + GPT(ë˜ëŠ” ê·œì¹™) ë‰´ìŠ¤ (ìºì‹±)")
    print("2. í…ŒìŠ¤íŠ¸ ë°ì´í„° + GPT(ë˜ëŠ” ê·œì¹™) ë‰´ìŠ¤ (ìºì‹±)")
    print("3. í† ìŠ¤ + ë°±ì—…/ë‹¨ìˆœ ë‰´ìŠ¤ (ìºì‹±)")
    print("4. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (5ê°œ ì¢…ëª©)")

    mode = input("\nì„ íƒ (1-4): ").strip() or "2"
    use_toss = mode in ["1", "3"]
    use_gpt  = mode in ["1", "2", "4"]
    quick    = mode == "4"

    driver = None
    if use_toss:
        driver = setup_driver()

    try:
        if quick:
            print("\nğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ìƒìœ„ 5ê°œ)")
            data = generate_test_data_with_cache()[:5]
            print("\n" + "="*50)
            print("ğŸ“ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            for s in data:
                print(f"\n{s['rank']}. {s['name']} ({s['rate']})")
                for ln in s['summary'].split('\n'):
                    print(f"   {ln}")
                if s.get("bullish_url"): print(f"   â†— í˜¸ì¬ ë§í¬: {s['bullish_url']}")
                if s.get("bearish_url"): print(f"   â†— ì•…ì¬ ë§í¬: {s['bearish_url']}")
            with open('test_news.json', 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                print("\nğŸ’¾ test_news.json ì €ì¥ ì™„ë£Œ")
            return

        cycle = 0
        error_count = 0

        if use_toss:
            url = 'https://www.tossinvest.com/?live-chart=heavy_soar'
            print(f"ğŸ“ ìµœì´ˆ ì ‘ì†: {url}")
            driver.get(url); time.sleep(5)
            print("âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\n")

        while True:
            cycle += 1
            print(f"\nâ° [{cycle}íšŒì°¨] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

            if cycle % 60 == 0:
                news_cache.cleanup()

            if use_toss:
                if not check_page_health(driver):
                    error_count += 1
                    print(f"âš ï¸ í˜ì´ì§€ ìƒíƒœ ì´ìƒ ({error_count}íšŒ)")
                    if error_count >= 3:
                        print("ğŸ”„ ìƒˆë¡œê³ ì¹¨"); driver.refresh(); time.sleep(5); error_count = 0
                else:
                    error_count = 0
                    print("ğŸ“¡ ì‹¤ì‹œê°„ DOM ì½ê¸°")

                soup = BeautifulSoup(driver.page_source, 'html.parser')
                if mode == "3":
                    # ê°•ì œ ë°±ì—…/ë‹¨ìˆœ
                    rows = soup.select('tr[data-tossinvest-log="RankingListRow"]') or soup.select('tbody tr')[:10]
                    data = []
                    for i, row in enumerate(rows[:10], 1):
                        name = None
                        tag = row.select_one('span[class*="60z0ev1"]')
                        if tag: name = tag.get_text(strip=True)
                        if not name:
                            for sp in row.select('span'):
                                tx = sp.get_text(strip=True)
                                if tx and '%' not in tx and 'ì›' not in tx and ',' not in tx and 2 <= len(tx) <= 20:
                                    name = tx; break
                        if not name: name = f"ì¢…ëª©{i}"
                        cached = news_cache.get(name)
                        if cached: res = cached
                        else:
                            res = {"summary": get_simple_summary(name), "bullish_url":"", "bearish_url":"", "sources":[]}
                            news_cache.set(name, res)
                        data.append({
                            "rank": i, "name": name, "price": "í™•ì¸ì¤‘", "rate": "+0.00%",
                            "summary": res["summary"],
                            "bullish_url": res.get("bullish_url",""),
                            "bearish_url": res.get("bearish_url",""),
                            "sources": res.get("sources", [])
                        })
                else:
                    data = parse_toss_data_with_cache(soup, use_gpt=True)
                if not data:
                    print("âš ï¸ ë°ì´í„° ì—†ìŒ â†’ í…ŒìŠ¤íŠ¸ë¡œ ëŒ€ì²´")
                    data = generate_test_data_with_cache()
            else:
                data = generate_test_data_with_cache()

            if data:
                send_to_api(data)
                with open('latest_stocks.json', 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    print("ğŸ’¾ latest_stocks.json ì €ì¥ ì™„ë£Œ")

                print("\n" + "="*50)
                print("ğŸ“ˆ TOP 3:")
                for s in data[:3]:
                    print(f"\n{s['rank']}. {s['name']} ({s['rate']})")
                    print(f"   {s['summary'].splitlines()[0]}")
                    if s.get("bullish_url"): print(f"   â†— í˜¸ì¬ ë§í¬: {s['bullish_url']}")

                print(f"\nğŸ“Š {news_cache.get_stats()}")
                print(f"ğŸ“ˆ ì‹œì¥ ìƒíƒœ: {'ğŸ”´ ì¥ì¤‘' if is_market_hours() else 'âš« ì¥ì™¸'}")

            interval = get_update_interval()
            print(f"\nâ³ {interval}ì´ˆ í›„ ì¬ìˆ˜ì§‘â€¦")
            time.sleep(interval)

    except KeyboardInterrupt:
        print("\nğŸ›‘ ì¢…ë£Œ")
        print(f"ğŸ“Š ìµœì¢… {news_cache.get_stats()}")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback; traceback.print_exc()
    finally:
        if driver:
            driver.quit()
        news_cache.save_cache()

# scraper.pyì˜ main() í•¨ìˆ˜ ëë¶€ë¶„ ìˆ˜ì •
if __name__ == "__main__":
    # Docker/Production í™˜ê²½ì—ì„œëŠ” ìë™ìœ¼ë¡œ ëª¨ë“œ ì„ íƒ
    import sys
    
    if len(sys.argv) > 1 or os.environ.get('DOCKER_ENV'):
        # Docker í™˜ê²½ì´ê±°ë‚˜ ì¸ìê°€ ìˆìœ¼ë©´ ìë™ ì‹¤í–‰
        print("ğŸš€ ìë™ ëª¨ë“œ: í† ìŠ¤ í¬ë¡¤ë§ ì‹œë„, ì‹¤íŒ¨ì‹œ í…ŒìŠ¤íŠ¸ ë°ì´í„°")
        
        driver = None
        try:
            driver = setup_driver()
            url = 'https://www.tossinvest.com/?live-chart=heavy_soar'
            print(f"ğŸ“ í† ìŠ¤ ì ‘ì† ì‹œë„: {url}")
            driver.get(url)
            time.sleep(5)
            
            # í˜ì´ì§€ ì²´í¬
            if check_page_health(driver):
                print("âœ… í† ìŠ¤ í˜ì´ì§€ ì •ìƒ ë¡œë“œ")
                # ì‹¤ì œ í¬ë¡¤ë§ ì½”ë“œ...
            else:
                raise Exception("í† ìŠ¤ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âš ï¸ í† ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëŒ€ì²´")
            if driver:
                driver.quit()
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            data = generate_test_data_with_cache()
            send_to_api(data)
    else:
        # ë¡œì»¬ì—ì„œëŠ” ê¸°ì¡´ ë©”ë‰´ ë°©ì‹
        main()
