# -*- coding: utf-8 -*-
"""
ì‹¤ì‹œê°„ ê¸‰ë“±ì£¼ + ì˜¤ëŠ˜ì ë‰´ìŠ¤ 2ì¤„ ìš”ì•½(ğŸŸ¢/ğŸ”´) - ìºì‹± & ë§í¬ ì œê³µ ë²„ì „
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

import os
import re
import time
import json
import random
import xml.etree.ElementTree as ET
import requests

from urllib.parse import quote_plus
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple, List, Union
from dotenv import load_dotenv

# =========================
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# =========================
load_dotenv()

API_URL = os.getenv("API_URL", "http://127.0.0.1:5001/api/update")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_TIMEOUT = float(os.getenv("OPENAI_TIMEOUT", "12"))
OPENAI_RETRIES = int(os.getenv("OPENAI_RETRIES", "2"))
MAX_LINE_LEN = int(os.getenv("NEWS_MAX_LINE_LEN", "60"))
CACHE_DURATION_MINUTES = int(os.getenv("NEWS_CACHE_MINUTES", "60"))

# =========================
# ìºì‹œ
# =========================
class NewsCache:
    def __init__(self, cache_duration_minutes: int = 60):
        self.cache: Dict[str, Tuple[Union[dict, str], datetime]] = {}
        self.cache_duration = timedelta(minutes=cache_duration_minutes)
        self.hit_count = 0
        self.miss_count = 0
        self.cache_file = "news_cache.json"
        self.load_cache()

    def load_cache(self):
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                now = datetime.now()
                for stock, (value, ts_str) in data.items():
                    ts = datetime.fromisoformat(ts_str)
                    if now - ts < self.cache_duration:
                        self.cache[stock] = (value, ts)
                print(f"ğŸ“¦ ìºì‹œ ë¡œë“œ: {len(self.cache)}ê°œ ì¢…ëª©")
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def save_cache(self):
        try:
            data = {stock: (value, ts.isoformat()) for stock, (value, ts) in self.cache.items()}
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get(self, stock_name: str) -> Optional[dict]:
        if stock_name in self.cache:
            value, cached_time = self.cache[stock_name]
            if datetime.now() - cached_time < self.cache_duration:
                if isinstance(value, str):
                    self.miss_count += 1
                    return None
                self.hit_count += 1
                remaining = self.cache_duration - (datetime.now() - cached_time)
                print(f"  ğŸ’¾ ìºì‹œ ì‚¬ìš©: {stock_name} (ë‚¨ì€ {remaining.seconds//60}ë¶„)")
                return value
            else:
                del self.cache[stock_name]
        self.miss_count += 1
        return None

    def set(self, stock_name: str, summary_or_obj):
        value = {"summary": summary_or_obj, "bullish_url": "", "bearish_url": "", "sources": []} \
                if isinstance(summary_or_obj, str) else summary_or_obj
        self.cache[stock_name] = (value, datetime.now())
        self.save_cache()
        print(f"  ğŸ’¾ ìºì‹œ ì €ì¥: {stock_name}")

    def get_stats(self) -> str:
        total = self.hit_count + self.miss_count
        hit_rate = (self.hit_count / total * 100) if total else 0
        return f"ìºì‹œ ì ì¤‘ë¥ : {hit_rate:.1f}% (ì ì¤‘:{self.hit_count}, ë¯¸ìŠ¤:{self.miss_count})"

    def cleanup(self):
        now = datetime.now()
        expired = [stock for stock, (_, ts) in self.cache.items() if now - ts >= self.cache_duration]
        for stock in expired:
            del self.cache[stock]
        if expired:
            print(f"ğŸ—‘ï¸ ë§Œë£Œ ìºì‹œ ì •ë¦¬: {len(expired)}ê°œ")
            self.save_cache()

news_cache = NewsCache(CACHE_DURATION_MINUTES)

# =========================
# ìœ í‹¸/ì‹œì¥ì‹œê°„
# =========================
def is_market_hours() -> bool:
    """í‰ì¼ 09:00~15:30 KST"""
    now = datetime.now()
    if now.weekday() >= 5:
        return False
    t = now.time()
    o = datetime.strptime("09:00", "%H:%M").time()
    c = datetime.strptime("15:30", "%H:%M").time()
    return o <= t <= c

def get_update_interval() -> int:
    """ì¥ì¤‘ 10ì´ˆ, ì¥ì™¸ 60ì´ˆ"""
    return 10 if is_market_hours() else 60

def _trim_line(s: str, max_len: int = MAX_LINE_LEN) -> str:
    s = re.sub(r"\s+", " ", s or "").strip()
    return (s[:max_len] + "â€¦") if len(s) > max_len else s

def _force_two_lines(text: str) -> str:
    lines = [ln.strip() for ln in (text or "").splitlines() if ln.strip()]
    green = next((l for l in lines if l.startswith("ğŸŸ¢")), None)
    red   = next((l for l in lines if l.startswith("ğŸ”´")), None)
    if not green:
        first = lines[0] if lines else "íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ"
        green = f"ğŸŸ¢ í˜¸ì¬: {_trim_line(first)}"
    if not red:
        second = lines[1] if len(lines) > 1 else "íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ"
        red = f"ğŸ”´ ì•…ì¬: {_trim_line(second)}"
    return f"{_trim_line(green)}\n{_trim_line(red)}"

# =========================
# í¬ë¡¬ ë“œë¼ì´ë²„ (ê°œì„ ëœ ë²„ì „)
# =========================
def setup_driver():
    print("ğŸŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
    
    options = Options()
    
    # ê¸°ë³¸ ì˜µì…˜
    options.add_argument('--headless=new')  # ìƒˆë¡œìš´ headless ëª¨ë“œ
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-setuid-sandbox')
    options.add_argument('--window-size=1920,1080')
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    options.add_argument('--memory-pressure-off')
    options.add_argument('--disable-background-timer-throttling')
    options.add_argument('--disable-renderer-backgrounding')
    
    # ì•ˆí‹° ë””í…ì…˜
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-features=VizDisplayCompositor')
    options.add_argument('--disable-web-security')
    
    # User-Agent ì„¤ì • (ìµœì‹  Chrome)
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36')
    
    # ì‹¤í—˜ì  ì˜µì…˜
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_experimental_option('prefs', {
        'profile.default_content_setting_values.notifications': 2,
        'profile.default_content_settings.popups': 0
    })
    
    try:
        # ChromeDriver ì„¤ì¹˜/ê²½ë¡œ ì„¤ì •
        try:
            service = Service(ChromeDriverManager().install())
            print("âœ… ChromeDriverManagerë¡œ ë“œë¼ì´ë²„ ì„¤ì¹˜ ì™„ë£Œ")
        except:
            service = Service('/usr/bin/chromedriver')
            print("âœ… ì‹œìŠ¤í…œ chromedriver ì‚¬ìš©")
        
        driver = webdriver.Chrome(service=service, options=options)
        
        # JavaScriptë¡œ webdriver ì†ì„± ìˆ¨ê¸°ê¸°
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
        driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko', 'en-US', 'en']})")
        
        print("âœ… Chrome ë“œë¼ì´ë²„ ì¤€ë¹„ ì™„ë£Œ")
        return driver
        
    except Exception as e:
        print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
        raise

# =========================
# í† ìŠ¤ í˜ì´ì§€ ì²´í¬ (ê°œì„ )
# =========================
def check_page_health(driver) -> bool:
    try:
        # í˜ì´ì§€ ì™„ì „ ë¡œë“œ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ë°ì´í„° í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        result = driver.execute_script("""
            const rows = document.querySelectorAll('tr[data-tossinvest-log="RankingListRow"]');
            const tbody = document.querySelector('tbody');
            return {
                ready: document.readyState === 'complete',
                hasData: rows.length > 0,
                hasTbody: tbody !== null,
                rowCount: rows.length,
                bodyText: document.body.innerText.substring(0, 200)
            }
        """)
        
        print(f"  í˜ì´ì§€ ìƒíƒœ: ready={result.get('ready')}, ë°ì´í„°í–‰={result.get('rowCount')}ê°œ")
        
        if not result.get('hasData'):
            print(f"  í˜ì´ì§€ ë‚´ìš© ì¼ë¶€: {result.get('bodyText')}")
            
        return bool(result.get('ready')) and bool(result.get('hasData'))
        
    except Exception as e:
        print(f"  í˜ì´ì§€ ì²´í¬ ì‹¤íŒ¨: {e}")
        return False

# =========================
# í† ìŠ¤ í¬ë¡¤ë§ (ê°œì„ )
# =========================
def scrape_toss_stocks(driver):
    """í† ìŠ¤ì—ì„œ ê¸‰ë“±ì£¼ ë°ì´í„° í¬ë¡¤ë§"""
    try:
        url = 'https://tossinvest.com/stocks/market/soaring'  # ë‹¤ë¥¸ URL ì‹œë„
        print(f"ğŸ“ í† ìŠ¤ ì ‘ì†: {url}")
        driver.get(url)
        
        # ì´ˆê¸° ë¡œë”© ëŒ€ê¸°
        print("  â³ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(3)
        
        # ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë“œ
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
        time.sleep(2)
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
        selectors = [
            'tr[data-tossinvest-log="RankingListRow"]',
            'tbody tr',
            'div[class*="stock-item"]',
            'a[href*="/stocks/"]'
        ]
        
        rows = None
        for selector in selectors:
            rows = soup.select(selector)
            if rows:
                print(f"  âœ… {len(rows)}ê°œ ì¢…ëª© ë°œê²¬ (ì…€ë ‰í„°: {selector})")
                break
        
        if not rows:
            # í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€ ì¶œë ¥í•˜ì—¬ ë””ë²„ê¹…
            print("  âŒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. HTML êµ¬ì¡° í™•ì¸:")
            print(soup.prettify()[:1000])
            return None
            
        return parse_toss_data_with_cache(soup, use_gpt=True)
        
    except Exception as e:
        print(f"âŒ í† ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

# =========================
# í† ìŠ¤ íŒŒì‹±
# =========================
def parse_toss_data_with_cache(soup, use_gpt=True):
    stocks = []
    rows = soup.select('tr[data-tossinvest-log="RankingListRow"]')
    
    if not rows:
        print("âš ï¸ ê¸°ë³¸ ì…€ë ‰í„° ì‹¤íŒ¨, ëŒ€ì²´ ì…€ë ‰í„° ì‹œë„...")
        rows = soup.select('tbody tr')[:10]
    
    if not rows:
        print("âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print(f"\nğŸ“Š {len(rows)}ê°œ ì¢…ëª© íŒŒì‹± ì‹œì‘")
    print(f"ğŸ“ˆ {news_cache.get_stats()}\n")

    for i, row in enumerate(rows[:10], 1):
        try:
            # ì¢…ëª©ëª… ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
            name = None
            
            # ë°©ë²• 1: í´ë˜ìŠ¤ëª…ìœ¼ë¡œ
            name_elem = row.select_one('span[class*="stock-name"], span[class*="60z0ev1"]')
            if name_elem:
                name = name_elem.get_text(strip=True)
            
            # ë°©ë²• 2: ë§í¬ì—ì„œ
            if not name:
                link = row.select_one('a[href*="/stocks/"]')
                if link:
                    name = link.get_text(strip=True)
            
            # ë°©ë²• 3: ëª¨ë“  spanì—ì„œ
            if not name:
                for sp in row.select('span'):
                    tx = sp.get_text(strip=True)
                    if tx and '%' not in tx and 'ì›' not in tx and ',' not in tx and 2 <= len(tx) <= 20:
                        name = tx
                        break

            # ê°€ê²©/ë“±ë½ë¥ 
            price_elements = row.select('span[class*="price"], span._1p5yqoh0')
            price = "ê°€ê²© í™•ì¸ì¤‘"
            rate = "+0.00%"
            
            if len(price_elements) >= 2:
                price = price_elements[0].get_text(strip=True)
                rate = price_elements[1].get_text(strip=True)
            elif len(price_elements) == 1:
                text = price_elements[0].get_text(strip=True)
                if 'ì›' in text:
                    price = text
                elif '%' in text:
                    rate = text

            if not name:
                name = f"ì¢…ëª©{i}"

            print(f"  {i}. {name} - {price} ({rate})")

            # ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            if use_gpt:
                result = get_gpt_news_with_context_cached(name, rate)
            else:
                result = {"summary": get_simple_summary(name), "bullish_url":"", "bearish_url":"", "sources":[]}

            stocks.append({
                "rank": i,
                "name": name,
                "price": price,
                "rate": rate,
                "summary": result["summary"],
                "bullish_url": result.get("bullish_url", ""),
                "bearish_url": result.get("bearish_url", ""),
                "sources": result.get("sources", [])
            })

            for ln in result["summary"].split('\n'):
                print(f"      {ln}")

        except Exception as e:
            print(f"  âŒ {i}ë²ˆ ì¢…ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")

    return stocks if stocks else None

# =========================
# ë‰´ìŠ¤ ìˆ˜ì§‘ & ìš”ì•½ (ê¸°ì¡´ í•¨ìˆ˜ë“¤)
# =========================
def fetch_google_news_today(stock_name: str, max_items: int = 6) -> List[dict]:
    q = quote_plus(f"{stock_name} when:1d")
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        resp = requests.get(url, headers=headers, timeout=4)
        resp.raise_for_status()
    except Exception:
        return []
    items = []
    try:
        root = ET.fromstring(resp.content)
        for it in root.findall(".//item"):
            title_el = it.find("title")
            link_el = it.find("link")
            title = (title_el.text if title_el is not None else "") or ""
            link = (link_el.text if link_el is not None else "") or ""
            if title:
                items.append({"title": title, "link": link, "published": ""})
            if len(items) >= max_items:
                break
    except Exception:
        return []
    return items

def summarize_with_gpt_from_headlines(stock_name: str, rate_text: str, headlines: List[dict]) -> dict:
    def _wrap(summary_text: str, bull_idx: Optional[int] = None, bear_idx: Optional[int] = None) -> dict:
        bull_url = headlines[bull_idx-1]["link"] if headlines and bull_idx and 1 <= bull_idx <= len(headlines) else ""
        bear_url = headlines[bear_idx-1]["link"] if headlines and bear_idx and 1 <= bear_idx <= len(headlines) else ""
        return {
            "summary": _force_two_lines(summary_text),
            "bullish_url": bull_url,
            "bearish_url": bear_url,
            "sources": headlines[:]
        }

    base = rule_based_summary(stock_name, rate_text, headlines)
    return _wrap(base, bull_idx=1 if len(headlines)>=1 else None,
                      bear_idx=2 if len(headlines)>=2 else None)

def rule_based_summary(stock_name: str, rate_text: str, headlines: List[dict]) -> str:
    if headlines:
        top = _trim_line(headlines[0]["title"])
        green = f"ğŸŸ¢ í˜¸ì¬: {top}"
        red = "ğŸ”´ ì•…ì¬: ë‹¨ê¸° ë³€ë™ì„±/ì°¨ìµì‹¤í˜„ ì£¼ì˜"
    else:
        green = "ğŸŸ¢ í˜¸ì¬: íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ"
        red = "ğŸ”´ ì•…ì¬: íŠ¹ë³„í•œ ë‰´ìŠ¤ ì—†ìŒ"
    return f"{_trim_line(green)}\n{_trim_line(red)}"

def get_gpt_news_with_context_cached(stock_name: str, current_rate: str) -> dict:
    cached = news_cache.get(stock_name)
    if cached:
        return cached
    print(f"  ğŸ”„ ìƒˆë¡œìš´ ë‰´ìŠ¤ ìš”ì²­: {stock_name}")
    headlines = fetch_google_news_today(stock_name)
    result = summarize_with_gpt_from_headlines(stock_name, current_rate, headlines)
    news_cache.set(stock_name, result)
    return result

def get_simple_summary(stock_name):
    samples = [
        f"ğŸŸ¢ í˜¸ì¬: {stock_name} ê±°ë˜ëŸ‰ ê¸‰ì¦\nğŸ”´ ì•…ì¬: ë‹¨ê¸° ê³¼ì—´ ì£¼ì˜",
        f"ğŸŸ¢ í˜¸ì¬: {stock_name} ê¸°ê´€ ìˆœë§¤ìˆ˜ ìœ ì…\nğŸ”´ ì•…ì¬: ì°¨ìµì‹¤í˜„ ë§¤ë¬¼ ê°€ëŠ¥",
        f"ğŸŸ¢ í˜¸ì¬: {stock_name} ì—…ì¢… ê°•ì„¸ ë™ì¡°\nğŸ”´ ì•…ì¬: ë³€ë™ì„± í™•ëŒ€ ì£¼ì˜"
    ]
    return random.choice(samples)

# =========================
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
# =========================
def generate_test_data_with_cache():
    test_stocks = [
        {"name": "ì‚¼ì„±ì „ì", "price": "87,500ì›", "rate": "+29.95%"},
        {"name": "SKí•˜ì´ë‹‰ìŠ¤", "price": "142,300ì›", "rate": "+25.32%"},
        {"name": "ì¹´ì¹´ì˜¤", "price": "58,900ì›", "rate": "+21.24%"},
        {"name": "ë„¤ì´ë²„", "price": "185,200ì›", "rate": "+18.56%"},
        {"name": "í˜„ëŒ€ì°¨", "price": "245,000ì›", "rate": "+15.87%"},
        {"name": "LGí™”í•™", "price": "485,000ì›", "rate": "+12.65%"},
        {"name": "í¬ìŠ¤ì½”í™€ë”©ìŠ¤", "price": "392,500ì›", "rate": "+10.93%"},
        {"name": "ì‚¼ì„±SDI", "price": "425,000ì›", "rate": "+9.54%"},
        {"name": "ì…€íŠ¸ë¦¬ì˜¨", "price": "178,900ì›", "rate": "+8.82%"},
        {"name": "ê¸°ì•„", "price": "115,200ì›", "rate": "+7.95%"}
    ]

    stocks = []
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {len(test_stocks)}ê°œ ì¢…ëª©")
    
    for i, st in enumerate(test_stocks, 1):
        print(f"  {i}. {st['name']} - {st['price']} ({st['rate']})")
        result = get_gpt_news_with_context_cached(st["name"], st["rate"])
        stocks.append({
            "rank": i,
            "name": st["name"],
            "price": st["price"],
            "rate": st["rate"],
            "summary": result["summary"],
            "bullish_url": result.get("bullish_url", ""),
            "bearish_url": result.get("bearish_url", ""),
            "sources": result.get("sources", [])
        })
    return stocks

# =========================
# API ì „ì†¡
# =========================
def send_to_api(data):
    try:
        resp = requests.post(API_URL, json=data, timeout=5)
        if resp.status_code == 200:
            print("âœ… API ì „ì†¡ ì„±ê³µ")
            return True
        else:
            print(f"âŒ API ì‘ë‹µ ì½”ë“œ: {resp.status_code}")
    except Exception as e:
        print(f"âŒ API ì „ì†¡ ì‹¤íŒ¨: {e}")
    return False

# =========================
# ë©”ì¸ (ìˆ˜ë™ ëª¨ë“œ)
# =========================
def main():
    print("=" * 50)
    print("ğŸš€ ì‹¤ì‹œê°„ ê¸‰ë“±ì£¼ + 2ì¤„ ë‰´ìŠ¤ & ë§í¬ (ìºì‹±)")
    print("=" * 50)
    # ... (ê¸°ì¡´ ìˆ˜ë™ ëª¨ë“œ ì½”ë“œ)

# =========================
# ìë™ ì‹¤í–‰ ëª¨ë“œ (Docker/Production)
# =========================
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 or os.environ.get('DOCKER_ENV'):
        print("ğŸš€ ìë™ ëª¨ë“œ ì‹¤í–‰ (Docker/Production)")
        print("=" * 60)
        
        cycle = 0
        consecutive_failures = 0
        
        while True:
            cycle += 1
            print(f"\nâ° [{cycle}íšŒì°¨] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("-" * 60)
            
            # ìºì‹œ ì •ë¦¬
            if cycle % 60 == 0:
                news_cache.cleanup()
            
            driver = None
            data = None
            
            try:
                # Chrome ë“œë¼ì´ë²„ ìƒì„±
                driver = setup_driver()
                
                # í† ìŠ¤ í¬ë¡¤ë§ ì‹œë„
                data = scrape_toss_stocks(driver)
                
                if data:
                    send_to_api(data)
                    print(f"âœ… í† ìŠ¤ ì‹¤ì‹œê°„ ë°ì´í„° {len(data)}ê°œ ì¢…ëª© ì „ì†¡ ì™„ë£Œ")
                    consecutive_failures = 0
                else:
                    raise Exception("í† ìŠ¤ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨")
                    
            except Exception as e:
                consecutive_failures += 1
                print(f"âš ï¸ í† ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨ (ì—°ì† {consecutive_failures}íšŒ): {e}")
                
                # 3íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ URL ì‹œë„
                if consecutive_failures >= 3:
                    print("ğŸ”„ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì‹œë„...")
                
                # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëŒ€ì²´
                print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëŒ€ì²´")
                data = generate_test_data_with_cache()
                if data:
                    send_to_api(data)
                    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° {len(data)}ê°œ ì¢…ëª© ì „ì†¡ ì™„ë£Œ")
                    
            finally:
                # ë“œë¼ì´ë²„ ì •ë¦¬
                if driver:
                    try:
                        driver.quit()
                        print("ğŸ§¹ ë“œë¼ì´ë²„ ì •ë¦¬ ì™„ë£Œ")
                    except:
                        pass
            
            # ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ ëŒ€ê¸°
            interval = get_update_interval()
            print(f"\nâ³ {interval}ì´ˆ í›„ ì¬ì‹¤í–‰...")
            print("=" * 60)
            time.sleep(interval)
            
    else:
        # ë¡œì»¬ ìˆ˜ë™ ëª¨ë“œ
        main()
